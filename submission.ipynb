{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28ec5a9-3838-4cdf-96cc-25357160119e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Constant Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485d0e3a-9dbf-4026-a8da-bac47d66fcc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from joblib import dump\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d515eb02-b0dd-4c84-bbe0-eafdff5c5c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "IMG_SIZE = (256, 256)\n",
    "IMG_DIR = './data/data'\n",
    "LABEL_TO_INT = {'NORMAL': 0, 'COVID': 1, 'PNEUMONIA': 2}\n",
    "INT_TO_LABEL = {v: k for k, v in LABEL_TO_INT.items()}\n",
    "\n",
    "_ = torch.manual_seed(SEED) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ff9ee-4e57-4b01-ac91-f8b1ef097fea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddba61b-8975-4a89-b86d-40eb40a3a6fb",
   "metadata": {},
   "source": [
    "Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aafde16-ee17-49d0-920c-bd900105e502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata, img_dir, img_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load images in memory\n",
    "        self.x, self.y = [], []\n",
    "        for _, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "            path = os.path.join(img_dir, row['image_id'])\n",
    "            img = Image.open(path)\n",
    "            if len(img.getbands()) != 3:\n",
    "                print(f\"Skipping {row['image_id']} because it does not have 3 channels.\")\n",
    "                continue\n",
    "            img = img.resize(img_size, Image.Resampling.BILINEAR)\n",
    "            self.x.append(img)\n",
    "            self.y.append(LABEL_TO_INT[row['label']])\n",
    "        \n",
    "        # Define the transforms used during training\n",
    "        self.transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.transforms(self.x[idx]), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd237631-5616-47fa-894f-bfee7ed529a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc3baa9d7e44ea1a22459be085d8081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 4210714.png because it does not have 3 channels.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "train_set = Dataset(train, IMG_DIR, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f9bcc-da9c-4c8c-89dd-a56f603d6987",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d999e0-1df2-46cf-a11d-719da1fdc1b9",
   "metadata": {},
   "source": [
    "We simply use some pretrained models. We just change the last layer to fit the number of possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d78af64-a125-42e3-b1aa-2261b04291ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, backbone_name, num_classes, bias):\n",
    "        super().__init__()\n",
    "        \n",
    "        if backbone_name == 'mobilenet_v2':\n",
    "            print('Using mobilenet_v2 as backbone.')\n",
    "            weights = torchvision.models.MobileNet_V2_Weights.IMAGENET1K_V1\n",
    "            self.backbone = torchvision.models.mobilenet_v2(\n",
    "                weights=weights,\n",
    "                progress=False,\n",
    "            )\n",
    "            self.backbone.classifier[-1] = torch.nn.Linear(\n",
    "                in_features=self.backbone.classifier[-1].in_features, out_features=num_classes, bias=bias\n",
    "            )\n",
    "        elif backbone_name == 'googlenet':\n",
    "            print('Using googlenet as backbone.')\n",
    "            weights = torchvision.models.GoogLeNet_Weights.IMAGENET1K_V1\n",
    "            self.backbone = torchvision.models.googlenet(\n",
    "                weights=weights,\n",
    "                progress=False,\n",
    "            )\n",
    "            self.backbone.fc = torch.nn.Linear(\n",
    "                in_features=self.backbone.fc.in_features, out_features=num_classes, bias=bias\n",
    "            )\n",
    "        elif backbone_name == 'efficientnet_b2':\n",
    "            print('Using efficientnet_b2 as backbone.')\n",
    "            weights = torchvision.models.EfficientNet_B2_Weights.IMAGENET1K_V1\n",
    "            self.backbone = torchvision.models.efficientnet_b2(\n",
    "                weights=weights,\n",
    "                progress=False,\n",
    "            )\n",
    "            self.backbone.classifier[-1] = torch.nn.Linear(\n",
    "                in_features=self.backbone.classifier[-1].in_features, out_features=num_classes, bias=bias\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(f\"Unknown backbone {backbone_name}.\")\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.backbone(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f2728-45e9-4525-a414-6562d2bf4fff",
   "metadata": {},
   "source": [
    "The final model is a stacking of a few trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f3f782-cbcf-41d7-bade-391de93d51be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stacking(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, models):\n",
    "        super().__init__()\n",
    "        self.models = models\n",
    "        self.n_models = len(self.models)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = None\n",
    "        with torch.no_grad():\n",
    "            for model in self.models:\n",
    "                model.eval()\n",
    "                if out is None:\n",
    "                    out = torch.softmax(model(inputs), dim=-1)\n",
    "                else:\n",
    "                    out += torch.softmax(model(inputs), dim=-1)\n",
    "        return out / self.n_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880aa7e0-c007-4665-a2f0-3732dec62499",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b89b32-da2f-4ff6-befe-f4c07f651561",
   "metadata": {},
   "source": [
    "This is where we actually train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49fc20a6-10b7-4a65-afad-75ecb388c9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit(dataset, backbone_name, device, batch_size, n_epochs, lr, bias):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    net = Net(backbone_name=backbone_name, num_classes=len(LABEL_TO_INT), bias=bias).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad], lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=int(.9 * n_epochs), gamma=.1)\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        net.train()\n",
    "        for i, (batch_x, batch_y) in enumerate(loader):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = loss_fn(net(batch_x), batch_y)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba9879b-63fb-4bb2-835e-9170871d2ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mobilenet_v2 as backbone.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb92dfdf0fb840e8b15c9e77b8df2acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using googlenet as backbone.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11be6d1a64504f88b15a1a8637e13510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using efficientnet_b2 as backbone.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6538eaacf64ee3a672fae414f4a09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['final_model.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "models = []\n",
    "for backbone_name in ['mobilenet_v2', 'googlenet', 'efficientnet_b2']:\n",
    "    model = fit(train_set, backbone_name, device, batch_size=64, n_epochs=40, lr=1e-4, bias=False)\n",
    "    dump(model, f\"{backbone_name}.joblib\")\n",
    "    models.append(model)\n",
    "\n",
    "final_model = Stacking(models)\n",
    "dump(final_model, \"final_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f48b62-ff62-4564-b6d2-624139de0844",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fdb761e-047f-4386-8b69-0bc423d5101f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5ede6f8066462589edcd3e009e02fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1569 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "transforms = torchvision.transforms.ToTensor()\n",
    "predictions = []\n",
    "for fname in tqdm(test['image_id']):\n",
    "    path = os.path.join(IMG_DIR, fname)\n",
    "    img = Image.open(path).resize(IMG_SIZE, Image.Resampling.BILINEAR)\n",
    "    img = transforms(img).to(device)\n",
    "    img = torch.unsqueeze(img, dim=0) # add batch dim\n",
    "    y_pred = torch.argmax(final_model(img)[0]).cpu().item()\n",
    "    predictions.append(INT_TO_LABEL[y_pred])\n",
    "\n",
    "test['label'] = predictions\n",
    "test[['trustii_id', 'label']].to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
